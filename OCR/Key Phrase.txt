import nltk
from nltk.corpus import stopwords
import string

nltk.download('punkt')
nltk.download('stopwords')

# Sample text for preprocessing
text = "Your text data goes here."

# Tokenize the text into words
tokens = nltk.word_tokenize(text)

# Convert tokens to lowercase and remove punctuation
tokens = [word.lower() for word in tokens if word.isalpha()]

# Remove stop words
stop_words = set(stopwords.words('english'))
filtered_tokens = [word for word in tokens if word not in stop_words]

# Join tokens back into a cleaned text string
cleaned_text = ' '.join(filtered_tokens)

# Display the cleaned text
print("Cleaned Text:")
print(cleaned_text)


# Add domain-specific stop words to the existing NLTK stop words list
custom_stop_words = set(['specific_word1', 'specific_word2'])  # Add your custom stop words here

# Merge custom stop words with NLTK's stop words list
stop_words = stop_words.union(custom_stop_words)

# Apply the updated stop words list in your text preprocessing
filtered_tokens = [word for word in tokens if word not in stop_words]

# Continue with the rest of the text preprocessing steps
# (lowercasing, removing punctuation, etc.)
