import nltk
from nltk.corpus import stopwords
import string

nltk.download('punkt')
nltk.download('stopwords')

# Sample text for preprocessing
text = "Your text data goes here."

# Tokenize the text into words
tokens = nltk.word_tokenize(text)

# Convert tokens to lowercase and remove punctuation
tokens = [word.lower() for word in tokens if word.isalpha()]

# Remove stop words
stop_words = set(stopwords.words('english'))
filtered_tokens = [word for word in tokens if word not in stop_words]

# Join tokens back into a cleaned text string
cleaned_text = ' '.join(filtered_tokens)

# Display the cleaned text
print("Cleaned Text:")
print(cleaned_text)


# Add domain-specific stop words to the existing NLTK stop words list
custom_stop_words = set(['specific_word1', 'specific_word2'])  # Add your custom stop words here

# Merge custom stop words with NLTK's stop words list
stop_words = stop_words.union(custom_stop_words)

# Apply the updated stop words list in your text preprocessing
filtered_tokens = [word for word in tokens if word not in stop_words]

# Continue with the rest of the text preprocessing steps
# (lowercasing, removing punctuation, etc.)


from rake_nltk import Rake

# Sample text for key phrase extraction
text = "Your text data goes here."

# Initialize Rake with custom parameters
r = Rake(min_length=1, max_length=2)  # Adjust min_length and max_length as needed

# Extract keywords and key phrases
r.extract_keywords_from_text(text)

# Get the ranked key phrases
key_phrases = r.get_ranked_phrases()

# Display the extracted key phrases
print("Extracted Key Phrases:")
for phrase in key_phrases:
    print(phrase)

