Sure, here's a Python function that performs the preprocessing tasks you mentioned: tokenization, lowercase conversion, punctuation removal, and stemming using the NLTK library.

```python
import string
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer

def preprocess_text(text):
    # Tokenization
    tokens = word_tokenize(text)
    
    # Lowercasing
    tokens = [token.lower() for token in tokens]
    
    # Removing punctuation
    table = str.maketrans('', '', string.punctuation)
    stripped = [token.translate(table) for token in tokens]
    
    # Removing stopwords
    stop_words = set(stopwords.words('english'))
    tokens = [token for token in stripped if token not in stop_words]
    
    # Stemming
    stemmer = PorterStemmer()
    stemmed_tokens = [stemmer.stem(token) for token in tokens]
    
    return stemmed_tokens

# Example usage:
text = "Hello! This is an example sentence for text preprocessing."
preprocessed_text = preprocess_text(text)
print(preprocessed_text)
```

This function takes a piece of text as input and performs the following preprocessing steps:
1. Tokenization: Split the text into individual words or tokens.
2. Lowercasing: Convert all tokens to lowercase.
3. Removing punctuation: Remove any punctuation marks from the tokens.
4. Removing stopwords: Remove common stopwords like 'the', 'is', 'and', etc.
5. Stemming: Reduce words to their root form using stemming (Porter Stemmer in this case).

You need to make sure to have NLTK installed and also download necessary NLTK resources by running `nltk.download('punkt')` and `nltk.download('stopwords')` before using this code if you haven't done so already.