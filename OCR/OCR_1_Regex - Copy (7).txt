Sure, here's a Python function that performs the preprocessing tasks you mentioned: tokenization, lowercase conversion, punctuation removal, and stemming using the NLTK library.

```python
import string
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer

def preprocess_text(text):
    # Tokenization
    tokens = word_tokenize(text)
    
    # Lowercasing
    tokens = [token.lower() for token in tokens]
    
    # Removing punctuation
    table = str.maketrans('', '', string.punctuation)
    stripped = [token.translate(table) for token in tokens]
    
    # Removing stopwords
    stop_words = set(stopwords.words('english'))
    tokens = [token for token in stripped if token not in stop_words]
    
    # Stemming
    stemmer = PorterStemmer()
    stemmed_tokens = [stemmer.stem(token) for token in tokens]
    
    return stemmed_tokens

# Example usage:
text = "Hello! This is an example sentence for text preprocessing."
preprocessed_text = preprocess_text(text)
print(preprocessed_text)
```

This function takes a piece of text as input and performs the following preprocessing steps:
1. Tokenization: Split the text into individual words or tokens.
2. Lowercasing: Convert all tokens to lowercase.
3. Removing punctuation: Remove any punctuation marks from the tokens.
4. Removing stopwords: Remove common stopwords like 'the', 'is', 'and', etc.
5. Stemming: Reduce words to their root form using stemming (Porter Stemmer in this case).

You need to make sure to have NLTK installed and also download necessary NLTK resources by running `nltk.download('punkt')` and `nltk.download('stopwords')` before using this code if you haven't done so already.




# LDA TOPIC WISE FULL CORPUS 


To get the full corpus of each individual LDA (Latent Dirichlet Allocation) topic, you need to extract the documents that are most strongly associated with each topic. Here's how you can do it using Gensim's LDA model:

```python
from gensim import models
from gensim.utils import simple_preprocess

# Assuming you have already created the dictionary and corpus
# dictionary and corpus creation steps are not shown here, assuming they are already created

# Train the LDA model
lda_model = models.LdaModel(corpus, num_topics=10, id2word=dictionary, passes=15)

# Get the top documents for each topic
num_top_documents = 10  # You can adjust the number of top documents you want to retrieve for each topic

for topic_id in range(lda_model.num_topics):
    print(f"Topic {topic_id}:")
    topic_documents = []
    for index, score in lda_model.get_topic_terms(topic_id):
        topic_documents.extend(lda_model.get_document_topics(corpus[index], minimum_probability=0.0))
    sorted_documents = sorted(topic_documents, key=lambda x: x[1], reverse=True)[:num_top_documents]
    for doc_id, score in sorted_documents:
        print(f"Document ID: {doc_id}, Score: {score}")
        print(processed_text[doc_id])  # print the actual document text from the processed_text list
        print()
    print("----------")
```

Explanation:

- We train an LDA model using `models.LdaModel()`.
- We loop through each topic and retrieve the top documents associated with that topic by first getting the topic terms and then retrieving documents that are associated with those terms.
- The `get_document_topics()` method retrieves the topic distribution for a given document.
- We sort the documents based on their association with the topic and print out the top documents along with their IDs and scores.

You may need to adjust the parameters like the number of topics (`num_topics`) and the number of top documents to retrieve (`num_top_documents`) based on your specific requirements and the characteristics of your data.



To save the results of the LDA model, including the document IDs, scores, and the actual document text, into an Excel file, you can use the `pandas` library. Here's how you can modify the previous code to save the results into an Excel file:

```python
import pandas as pd
from gensim import models
from gensim.utils import simple_preprocess

# Assuming you have already created the dictionary and corpus
# dictionary and corpus creation steps are not shown here, assuming they are already created

# Train the LDA model
lda_model = models.LdaModel(corpus, num_topics=10, id2word=dictionary, passes=15)

# Get the top documents for each topic
num_top_documents = 10  # You can adjust the number of top documents you want to retrieve for each topic

# Create an empty list to store the results
results = []

for topic_id in range(lda_model.num_topics):
    topic_documents = []
    for index, score in lda_model.get_topic_terms(topic_id):
        topic_documents.extend(lda_model.get_document_topics(corpus[index], minimum_probability=0.0))
    sorted_documents = sorted(topic_documents, key=lambda x: x[1], reverse=True)[:num_top_documents]
    for doc_id, score in sorted_documents:
        document_text = ' '.join(processed_text[doc_id])  # Joining the tokenized text into a single string
        results.append([topic_id, doc_id, score, document_text])

# Convert results to a DataFrame
df = pd.DataFrame(results, columns=['Topic ID', 'Document ID', 'Score', 'Document Text'])

# Save DataFrame to Excel
excel_file_output = 'lda_results.xlsx'
df.to_excel(excel_file_output, index=False)
print(f"Results saved to {excel_file_output}")
```

This code iterates over each topic, retrieves the top documents associated with that topic, and then stores the results in a list. After processing all topics, it converts the list into a pandas DataFrame and saves it to an Excel file using the `to_excel()` method.

Make sure to adjust the file paths and filenames according to your needs. This code will create an Excel file named 'lda_results.xlsx' containing the results of the LDA model.