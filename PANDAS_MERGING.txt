import pandas as pd

# Create sample DataFrames
df1 = pd.DataFrame({'key': ['K0', 'K1', 'K2', 'K3'],
                    'A': ['A0', 'A1', 'A2', 'A3'],
                    'B': ['B0', 'B1', 'B2', 'B3']})

df2 = pd.DataFrame({'key': ['K0', 'K1', 'K2', 'K4'],
                    'C': ['C0', 'C1', 'C2', 'C4'],
                    'D': ['D0', 'D1', 'D2', 'D4']})

# Inner merge
inner_merged = pd.merge(df1, df2, on='key', how='inner')
print("Inner Merge:")
print(inner_merged)

# Left merge
left_merged = pd.merge(df1, df2, on='key', how='left')
print("\nLeft Merge:")
print(left_merged)

# Right merge
right_merged = pd.merge(df1, df2, on='key', how='right')
print("\nRight Merge:")
print(right_merged)

# Outer merge
outer_merged = pd.merge(df1, df2, on='key', how='outer')
print("\nOuter Merge:")
print(outer_merged)



import pandas as pd

# Create sample DataFrames
df1 = pd.DataFrame({'key': ['K0', 'K1', 'K2', 'K3'],
                    'A': ['A0', 'A1', 'A2', 'A3'],
                    'B': ['B0', 'B1', 'B2', 'B3']})

df2 = pd.DataFrame({'key': ['K0', 'K1', 'K2', 'K3'],
                    'C': ['C0', 'C1', 'C2', 'C3'],
                    'D': ['D0', 'D1', 'D2', 'D3']})

# Merging based on a condition
condition = 'A'  # Column to base the condition on
value = 'A1'  # Value to check in the condition column

# Filter rows in df1 based on the condition
filtered_df1 = df1[df1[condition] == value]

# Perform the merge
merged_df = pd.merge(filtered_df1, df2, on='key', how='inner')

# Display the merged DataFrame
print("Merged DataFrame based on the condition:")
print(merged_df)


HR_ANALYTICE COMPLEX LOGIC


Certainly! Here's a Python code snippet using pandas to implement the described logic:

```python
import pandas as pd

# Assuming df is your DataFrame
# Sample DataFrame with 7 columns
data = {
    'A': [1, 2, 3, 4],
    'B': [5, 6, 7, 8],
    'C': [9, 10, 11, 12],
    'D': [13, 14, 15, 16],
    'E': [17, 18, 19, 20],
    'F': [21, 22, 23, 24],
    'attribute_side': [25, 26, 27, 28]
}

df = pd.DataFrame(data)

# Function to generate consecutive numbers starting from 2
def generate_consecutive_numbers(n):
    return list(range(2, n + 2))

# Iterate for 3 years starting from September 2023
for year in range(2023, 2026):
    for month in range(1, 13):
        # Generate consecutive numbers starting from 2
        consecutive_numbers = generate_consecutive_numbers(len(df))

        # Create a new column "attr_month_year"
        df[f'attr_month_year_{month}_{year}'] = df['attribute_side'] * consecutive_numbers

        # Implement logic for subtraction
        condition = (df['A'] == df['B']) & (df['B'] == df['C']) & (df['C'] == df['D'])
        subtracted_values = df.loc[condition, 'B'] - df.loc[condition, f'attr_month_year_{month}_{year}']

        # Capture lines where subtraction occurs
        subtracted_values_index = subtracted_values.index
        df.loc[subtracted_values_index, 'subtract_result'] = subtracted_values

# Display the resulting DataFrame
print(df)
```

This code iterates through each month and year, creating a new column "attr_month_year" and applying the specified logic for subtraction. The result is captured in the "subtract_result" column. Adjust the logic as needed based on your actual DataFrame structure and requirements.






To update the code for pandas version 0.23.4, we need to consider the methods and functions available in that version. Here is the updated code:

1. `json_normalize` should be imported from `pandas.io.json` as it's not directly part of pandas.
2. `explode` method does not exist in pandas 0.23.4, so we need to use an alternative method to achieve the same functionality.

Below is the modified code:

```python
import pandas as pd
from pandas.io.json import json_normalize
from pymongo import MongoClient

# Assuming COLLECTION_CASE is a MongoDB collection
# df_case = pd.DataFrame(list(COLLECTION_CASE.find(filter_criteria, projection)))
# For illustration, let's assume df_case is already defined.

# Normalize the JSON data
df_normalized = json_normalize(df_case.to_dict('records'))

# Explode 'serviceRequests' column
df_normalized = df_normalized.set_index(df_normalized.columns.drop('serviceRequests', 1).tolist())
df_normalized = df_normalized['serviceRequests'].apply(pd.Series).stack().reset_index().rename(columns={0:'serviceRequests'})

# Merge back with the rest of the DataFrame
df_normalized = df_normalized.join(df_case, on='level_0').drop(['level_0', 'serviceRequests_y'], axis=1).rename(columns={'serviceRequests_x': 'serviceRequests'})

# Further normalization if needed
df_normalized_final = json_normalize(df_normalized.to_dict('records'))

# Display the result
df_normalized_final.head()
```

Key changes and explanations:
1. Importing `json_normalize` from `pandas.io.json`.
2. Since `explode` is not available, we manually "explode" the `serviceRequests` column by setting the other columns as the index, applying `pd.Series` to `serviceRequests`, using `stack()`, and then resetting the index.
3. Joining the exploded DataFrame back with the original DataFrame to retain all other columns.

Make sure that `df_case` is defined with the relevant data, as this example assumes it is already created from a MongoDB query.